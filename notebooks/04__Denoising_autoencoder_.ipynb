{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_<Denoising autoencoder>.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAhkEVhxwnVf"
      },
      "source": [
        "# Applications of deep neural network for denoising data\n",
        "\n",
        "## Denoising Autoencoder with Keras libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECxXptFFw0wO"
      },
      "source": [
        "**First the required libraries are imported**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEEdM0tGlJEx"
      },
      "source": [
        "import numpy as np\n",
        "# import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYE67b2bszWT"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E98dRA2uspSZ"
      },
      "source": [
        "Here we import data with numpy library from github. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUbophJ0lRUv"
      },
      "source": [
        "url= 'https://raw.githubusercontent.com/Alborz2020/Workshop/main/Data.csv'\n",
        "Data = np.genfromtxt(url, delimiter=',',skip_header=1)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22LeywhiVnCv"
      },
      "source": [
        "Data needs to be normalized at the begining. we use MinMaxScaler function which normalizes our data set between 0 and 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWzbLZbRli_v"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(Data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssD1BevyWCiV"
      },
      "source": [
        "we assing 75% percent of whole data for training and the rest for testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OZTPO6DWBkn"
      },
      "source": [
        "A_train = data_scaled[:750]\n",
        "A_test = data_scaled[750:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8l5cxYjWZ9X"
      },
      "source": [
        " we are adding some normally distributed noises to our data. \"np.random.choice\" picks some indexs randomly to which we want to add noise \n",
        "\n",
        " the first number represent the total number of samples that random sample is generated from its elements\n",
        "\n",
        " the second number is the size of random numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXk5lbzapMw5"
      },
      "source": [
        "idt = np.random.choice(750, 100, replace=False) # idt is the number of noise for training samples\n",
        "idx = np.random.choice(250, 30, replace=False)  # idx is the number of noise for testing samples"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfTzMjzwjtP"
      },
      "source": [
        "to add  noises to our data sets. we first "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNsNHagtr5x0"
      },
      "source": [
        "Train_noisy= A_train\n",
        "Test_noisy = A_test\n",
        "Train_noisy[idt] = Train_noisy[idt] + np.random.normal(0,0.025,(Train_noisy[idt].shape[0],Train_noisy[idt].shape[1]))\n",
        "Test_noisy[idx] = Test_noisy[idx] + np.random.normal(0,0.025,(Test_noisy[idx].shape[0],Test_noisy[idx].shape[1]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG6rmKocr82t"
      },
      "source": [
        "input_data = Input(shape=(A_train.shape[1],))\n",
        "encoded=Dense(5, activation='tanh')(input_data)\n",
        "encoded=Dense(4, activation='tanh')(encoded)\n",
        "encoded=Dense(4, activation='tanh')(encoded)\n",
        "hidden=Dense(3, activation='tanh')(encoded)  \n",
        "decoded=Dense(4, activation='tanh')(hidden)\n",
        "decoded=Dense(4, activation='tanh')(decoded)\n",
        "decoded=Dense(5, activation='tanh')(decoded)\n",
        "output=Dense(A_train.shape[1])(decoded)\n",
        "autoencoder= Model(input_data,output)  \n",
        "autoencoder.compile(loss='mean_squared_error', optimizer='adam')\n",
        "history=autoencoder.fit(Train_noisy,A_train, verbose=1 , epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSEYxGcusAjN",
        "outputId": "660a9e2b-b7c7-4adf-c893-99aa86f4e3c8"
      },
      "source": [
        "predictions=autoencoder.predict(Test_noisy)\n",
        "score = (mean_squared_error(A_test, predictions))\n",
        "score"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00794389132118776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5P9s3qNsKf-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1b2bz6lsMSc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}